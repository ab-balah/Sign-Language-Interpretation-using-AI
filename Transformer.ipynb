{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7108432,"sourceType":"datasetVersion","datasetId":4098364}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Abderrahmane Balah - 201932370 - Section 1","metadata":{"id":"C6rKBw6Zb4c9"}},{"cell_type":"markdown","source":"This notebook has code related to developing a TRANSFORMER BASED MODEL.","metadata":{}},{"cell_type":"markdown","source":"# TRANSFORMER BASED SOLUTION FOR SIGN LANGUAGE INTERPRETATION","metadata":{}},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"id":"uWFTHi1Ab4BN"}},{"cell_type":"code","source":"import torch\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\nclass SignLanguageDataset(Dataset):\n    def __init__(self, root_dir, caption_mapping, vocab, transform=None, data=None):\n        self.root_dir = root_dir\n        self.caption_mapping = caption_mapping\n        self.vocab = vocab\n        self.transform = transform\n        if(data is None):\n          self.data = self._load_data()\n        else:\n          self.data = data\n\n    def _load_data(self):\n        data = []\n        for folder_name in sorted(os.listdir(self.root_dir)):\n            folder_path = os.path.join(self.root_dir, folder_name)\n            for video_folder in os.listdir(folder_path):\n                video_folder_path = os.path.join(folder_path, video_folder)\n                frames = [os.path.join(video_folder_path, frame) for frame in sorted(os.listdir(video_folder_path))]\n                caption = self.caption_mapping[folder_name]\n                numericalized_caption = numericalize(caption, self.vocab)\n                data.append((frames, numericalized_caption))\n        return data\n\n    def __getitem__(self, idx):\n        frames_paths, numericalized_caption = self.data[idx]\n        frames = [Image.open(frame_path).convert('RGB') for frame_path in frames_paths]\n\n        if self.transform:\n            frames = [self.transform(frame) for frame in frames]\n        else:\n            to_tensor = transforms.ToTensor()\n            frames = [to_tensor(frame) for frame in frames]\n\n        frames_tensor = torch.stack(frames)\n        caption_tensor = torch.tensor(numericalized_caption)\n        return frames_tensor, caption_tensor\n\n    def __len__(self):\n        return len(self.data)\n\n    def subset(self, indices):\n        subset_data = [self.data[i] for i in indices]\n        return SignLanguageDataset(self.root_dir, self.caption_mapping, self.vocab, self.transform, subset_data)\n\ndef load_captions(captions_file):\n    with open(captions_file, 'r', encoding='utf-8') as file:\n        captions = file.read().splitlines()\n    # Create a mapping from folder number to caption\n    caption_mapping = {str(i).zfill(4): caption for i, caption in enumerate(captions, 1)}\n    return caption_mapping\n\ndef build_vocab(captions):\n    tokenized_captions = [caption.lower().split() for caption in captions]\n    vocab = build_vocab_from_iterator(tokenized_captions, specials=['<unk>', '<pad>', '<sos>', '<eos>'])\n    vocab.set_default_index(vocab['<unk>'])\n    return vocab\n\ndef numericalize(caption, vocab):\n    return [vocab['<sos>']] + [vocab[token] for token in caption.lower().split()] + [vocab['<eos>']]\n\ndef collate_fn(batch):\n    # Separate frames and captions\n    frames, captions = zip(*batch)\n    # Pad the captions to the same length\n    captions_padded = pad_sequence(captions, batch_first=True, padding_value=vocab['<eos>'])\n    # Stack frames\n    frames_stacked = torch.stack(frames)\n    return frames_stacked, captions_padded\n\n# Load the data with the collate function\ndef get_data_loader(root_dir, caption_mapping, vocab, batch_size, transform, shuffle=True, num_workers=4):\n    dataset = SignLanguageDataset(root_dir=root_dir, caption_mapping=caption_mapping, vocab=vocab, transform=transform)\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)\n\n","metadata":{"id":"lxLv91KoceRs","execution":{"iopub.status.busy":"2023-12-09T19:55:45.514813Z","iopub.execute_input":"2023-12-09T19:55:45.515407Z","iopub.status.idle":"2023-12-09T19:55:47.371393Z","shell.execute_reply.started":"2023-12-09T19:55:45.515371Z","shell.execute_reply":"2023-12-09T19:55:47.370626Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"caption_mapping = load_captions('../input/assignment3/Assignment03/groundTruth.txt')\nall_captions = list(caption_mapping.values())\nvocab = build_vocab(all_captions)\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((256, 256)),  \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  \n])\ntest_transform = transforms.Compose([\n    transforms.Resize((256, 256)), \n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) \n])\n\ntest_data_loader = get_data_loader(\n    root_dir='../input/assignment3/Assignment03/test/',\n    caption_mapping=caption_mapping,\n    vocab=vocab,\n    batch_size=1,\n    transform=test_transform,\n    shuffle=True,\n    num_workers=2\n)\n","metadata":{"id":"YsIa_tp2gZxd","execution":{"iopub.status.busy":"2023-12-09T19:55:47.372837Z","iopub.execute_input":"2023-12-09T19:55:47.373256Z","iopub.status.idle":"2023-12-09T19:55:48.042261Z","shell.execute_reply.started":"2023-12-09T19:55:47.373231Z","shell.execute_reply":"2023-12-09T19:55:48.041494Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfull_test_dataset = SignLanguageDataset(root_dir='../input/assignment3/Assignment03/train/',\n                                        caption_mapping=caption_mapping,\n                                        vocab=vocab,\n                                        transform=train_transform)\n\ncaptions = [str(full_test_dataset.data[i][1]) for i in range(len(full_test_dataset))]\n\ntrain_indices, test_indices = train_test_split(range(len(full_test_dataset)),\n                                               stratify=captions,\n                                               test_size=0.2)  \n\ntrain_dataset = full_test_dataset.subset(train_indices)\nval_dataset = full_test_dataset.subset(test_indices)\n\ntrain_data_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\nval_data_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2, collate_fn=collate_fn)","metadata":{"id":"xDld9XmUr5VI","execution":{"iopub.status.busy":"2023-12-09T19:56:09.230291Z","iopub.execute_input":"2023-12-09T19:56:09.231189Z","iopub.status.idle":"2023-12-09T19:56:12.498282Z","shell.execute_reply.started":"2023-12-09T19:56:09.231158Z","shell.execute_reply":"2023-12-09T19:56:12.497427Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{"id":"_VP8tUc6kZX0"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.models.mobilenetv2 import MobileNet_V2_Weights","metadata":{"id":"rCSGZiUzkmBz","execution":{"iopub.status.busy":"2023-12-09T19:56:15.440265Z","iopub.execute_input":"2023-12-09T19:56:15.440680Z","iopub.status.idle":"2023-12-09T19:56:15.445419Z","shell.execute_reply.started":"2023-12-09T19:56:15.440645Z","shell.execute_reply":"2023-12-09T19:56:15.444530Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Transformer","metadata":{"id":"nKKUX71MkgwH"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\nimport math\nclass TransformerModel(nn.Module):\n    def __init__(self, feature_size, num_layers, num_heads, hidden_dim, vocab_size, max_seq_length, dropout=0.5):\n        super(TransformerModel, self).__init__()\n        self.feature_size = feature_size\n        mobilenet_weights = models.MobileNet_V2_Weights.DEFAULT\n        mobilenet = models.mobilenet_v2(weights=mobilenet_weights)\n        modules = list(mobilenet.children())[:-1]\n        self.mobilenet = nn.Sequential(*modules)\n        for param in self.mobilenet.parameters():\n            param.requires_grad = False\n\n        self.linear_cnn = nn.Linear(mobilenet.last_channel*8*8, feature_size)\n        self.pos_encoder = PositionalEncoding(feature_size, dropout)\n\n        # Transformer Encoder\n        encoder_layers = TransformerEncoderLayer(d_model=feature_size, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=num_layers)\n\n        # Transformer Decoder\n        self.embed = nn.Embedding(vocab_size, feature_size)\n        decoder_layers = TransformerDecoderLayer(d_model=feature_size, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n        self.transformer_decoder = TransformerDecoder(decoder_layers, num_layers=num_layers)\n        self.linear_vocab = nn.Linear(feature_size, vocab_size)\n\n    def forward(self, frames, captions):\n        # Extract and transform features from frames\n        batch_size, num_frames, C, H, W = frames.size()\n        frames = frames.view(batch_size * num_frames, C, H, W)\n        cnn_features = self.mobilenet(frames)\n        cnn_features = cnn_features.view(batch_size, num_frames, -1)\n        cnn_features = self.linear_cnn(cnn_features)\n        cnn_features = self.pos_encoder(cnn_features)\n        \n        # Transformer Encoder\n        encoder_output = self.transformer_encoder(cnn_features)\n\n        # Prepare captions for Transformer Decoder\n        captions = self.embed(captions) * math.sqrt(self.feature_size)\n        captions = self.pos_encoder(captions)\n        decoder_output = self.transformer_decoder(captions, encoder_output)\n\n        # Final Linear layer\n        output = self.linear_vocab(decoder_output)\n        return output\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n","metadata":{"id":"OZcorePDkY-9","execution":{"iopub.status.busy":"2023-12-09T20:12:24.209299Z","iopub.execute_input":"2023-12-09T20:12:24.209750Z","iopub.status.idle":"2023-12-09T20:12:24.231674Z","shell.execute_reply.started":"2023-12-09T20:12:24.209711Z","shell.execute_reply":"2023-12-09T20:12:24.230440Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"## Validation Function","metadata":{"id":"hfxJr6silgdC"}},{"cell_type":"code","source":"def validate_model(model, val_data_loader, criterion, device):\n    model.eval()\n    total_val_loss = 0\n    with torch.no_grad():\n        for frames, captions in val_data_loader:\n            frames, captions = frames.to(device), captions.to(device)\n            outputs = model(frames, captions[:, :-1])\n            loss = criterion(outputs.view(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n            total_val_loss += loss.item()\n    return total_val_loss / len(val_data_loader)\n","metadata":{"id":"8G7S6CnjtupT","execution":{"iopub.status.busy":"2023-12-09T20:12:24.622884Z","iopub.execute_input":"2023-12-09T20:12:24.623531Z","iopub.status.idle":"2023-12-09T20:12:24.629682Z","shell.execute_reply.started":"2023-12-09T20:12:24.623500Z","shell.execute_reply":"2023-12-09T20:12:24.628669Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Train Loop","metadata":{"id":"yUdQOsLelhct"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef train(model, model_filename, train_data_loader, val_data_loader, device, vocab, num_epochs=10):\n    model.to(device)\n\n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n    best_val_loss = float('inf')\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n\n        for batch_idx, (frames, captions) in enumerate(train_data_loader):\n            frames, captions = frames.to(device), captions.to(device)\n\n            # Zero the gradients\n            optimizer.zero_grad()\n\n            # Forward pass\n            outputs = model(frames, captions[:, :-1])  # Exclude the <eos> token from the input\n\n            # Calculate the loss\n            loss = criterion(outputs.reshape(-1, outputs.size(-1)), captions[:, 1:].contiguous().view(-1))\n            total_loss += loss.item()\n\n            # Backward pass and optimize\n            loss.backward()\n            optimizer.step()\n\n            if batch_idx % 10 == 0:  # Print loss every 10 batches\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}\")\n\n        # Print epoch loss\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_data_loader):.4f}')\n\n        # Validation phase\n        val_loss = validate_model(model, val_data_loader, criterion, device)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n\n        # Check for early stopping\n        if val_loss < best_val_loss:\n            print(f'Saving Model! {val_loss} < {best_val_loss}')\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), model_filename)\n","metadata":{"id":"nFyW-OmVLetf","execution":{"iopub.status.busy":"2023-12-09T20:14:33.788545Z","iopub.execute_input":"2023-12-09T20:14:33.788939Z","iopub.status.idle":"2023-12-09T20:14:33.801177Z","shell.execute_reply.started":"2023-12-09T20:14:33.788903Z","shell.execute_reply":"2023-12-09T20:14:33.800135Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"### Training Transformer","metadata":{"id":"F2kk-5PxkTLm"}},{"cell_type":"code","source":"feature_size = 512 \nnum_layers = 3   \nnum_heads = 8     \nhidden_dim = 2048  \nvocab_size = len(vocab)  \nmax_seq_length = 20 \n\n# Initialize the Transformer model\ntransformer_model = TransformerModel(feature_size, num_layers, num_heads, hidden_dim, vocab_size, max_seq_length, dropout=0.5)\n\n# Check if CUDA is available and move the model to GPU if possible\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntransformer_model.to(device)\n\n# Start training the Transformer model\nmodel_filename = \"transformer_model_v3.pth\"  # File name to save the model\ntrain(transformer_model, model_filename, train_data_loader, val_data_loader, device, vocab, num_epochs=10)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UdDBd5SYkS4F","outputId":"1ac76b29-cb0b-442a-e39a-7f4b3250c039","execution":{"iopub.status.busy":"2023-12-09T20:14:34.246996Z","iopub.execute_input":"2023-12-09T20:14:34.247352Z","iopub.status.idle":"2023-12-09T20:28:58.256127Z","shell.execute_reply.started":"2023-12-09T20:14:34.247322Z","shell.execute_reply":"2023-12-09T20:28:58.254901Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Epoch [1/10], Step [1/107], Loss: 3.7180\nEpoch [1/10], Step [11/107], Loss: 2.4077\nEpoch [1/10], Step [21/107], Loss: 2.4192\nEpoch [1/10], Step [31/107], Loss: 3.1973\nEpoch [1/10], Step [41/107], Loss: 2.4625\nEpoch [1/10], Step [51/107], Loss: 2.1275\nEpoch [1/10], Step [61/107], Loss: 2.0370\nEpoch [1/10], Step [71/107], Loss: 1.8708\nEpoch [1/10], Step [81/107], Loss: 1.8195\nEpoch [1/10], Step [91/107], Loss: 1.7743\nEpoch [1/10], Step [101/107], Loss: 1.8511\nEpoch [1/10], Loss: 2.1932\nEpoch [1/10], Validation Loss: 0.5412\nSaving Model! 0.5411722356470946 < inf\nEpoch [2/10], Step [1/107], Loss: 1.2802\nEpoch [2/10], Step [11/107], Loss: 0.9567\nEpoch [2/10], Step [21/107], Loss: 0.7685\nEpoch [2/10], Step [31/107], Loss: 0.6618\nEpoch [2/10], Step [41/107], Loss: 0.7344\nEpoch [2/10], Step [51/107], Loss: 0.7501\nEpoch [2/10], Step [61/107], Loss: 0.6570\nEpoch [2/10], Step [71/107], Loss: 0.7104\nEpoch [2/10], Step [81/107], Loss: 0.5879\nEpoch [2/10], Step [91/107], Loss: 0.4541\nEpoch [2/10], Step [101/107], Loss: 0.5287\nEpoch [2/10], Loss: 0.7461\nEpoch [2/10], Validation Loss: 0.1931\nSaving Model! 0.19306122945081966 < 0.5411722356470946\nEpoch [3/10], Step [1/107], Loss: 0.9806\nEpoch [3/10], Step [11/107], Loss: 0.4519\nEpoch [3/10], Step [21/107], Loss: 0.2720\nEpoch [3/10], Step [31/107], Loss: 0.4939\nEpoch [3/10], Step [41/107], Loss: 0.2825\nEpoch [3/10], Step [51/107], Loss: 0.5398\nEpoch [3/10], Step [61/107], Loss: 0.2157\nEpoch [3/10], Step [71/107], Loss: 0.2881\nEpoch [3/10], Step [81/107], Loss: 0.2687\nEpoch [3/10], Step [91/107], Loss: 0.2154\nEpoch [3/10], Step [101/107], Loss: 0.2384\nEpoch [3/10], Loss: 0.3281\nEpoch [3/10], Validation Loss: 0.0266\nSaving Model! 0.02664822969742328 < 0.19306122945081966\nEpoch [4/10], Step [1/107], Loss: 0.2231\nEpoch [4/10], Step [11/107], Loss: 0.1545\nEpoch [4/10], Step [21/107], Loss: 0.2201\nEpoch [4/10], Step [31/107], Loss: 0.1612\nEpoch [4/10], Step [41/107], Loss: 0.1334\nEpoch [4/10], Step [51/107], Loss: 0.1669\nEpoch [4/10], Step [61/107], Loss: 0.1381\nEpoch [4/10], Step [71/107], Loss: 0.1043\nEpoch [4/10], Step [81/107], Loss: 0.1145\nEpoch [4/10], Step [91/107], Loss: 0.0977\nEpoch [4/10], Step [101/107], Loss: 0.0960\nEpoch [4/10], Loss: 0.1543\nEpoch [4/10], Validation Loss: 0.0072\nSaving Model! 0.007227218015746118 < 0.02664822969742328\nEpoch [5/10], Step [1/107], Loss: 0.0724\nEpoch [5/10], Step [11/107], Loss: 0.0816\nEpoch [5/10], Step [21/107], Loss: 0.0724\nEpoch [5/10], Step [31/107], Loss: 0.0562\nEpoch [5/10], Step [41/107], Loss: 0.0794\nEpoch [5/10], Step [51/107], Loss: 0.0662\nEpoch [5/10], Step [61/107], Loss: 0.1296\nEpoch [5/10], Step [71/107], Loss: 0.0931\nEpoch [5/10], Step [81/107], Loss: 0.0737\nEpoch [5/10], Step [91/107], Loss: 0.0483\nEpoch [5/10], Step [101/107], Loss: 0.0370\nEpoch [5/10], Loss: 0.0871\nEpoch [5/10], Validation Loss: 0.0034\nSaving Model! 0.0034352220000838426 < 0.007227218015746118\nEpoch [6/10], Step [1/107], Loss: 0.0596\nEpoch [6/10], Step [11/107], Loss: 0.1097\nEpoch [6/10], Step [21/107], Loss: 0.0539\nEpoch [6/10], Step [31/107], Loss: 0.0532\nEpoch [6/10], Step [41/107], Loss: 0.0730\nEpoch [6/10], Step [51/107], Loss: 0.0592\nEpoch [6/10], Step [61/107], Loss: 0.0568\nEpoch [6/10], Step [71/107], Loss: 0.0319\nEpoch [6/10], Step [81/107], Loss: 0.0577\nEpoch [6/10], Step [91/107], Loss: 0.0473\nEpoch [6/10], Step [101/107], Loss: 0.0381\nEpoch [6/10], Loss: 0.0530\nEpoch [6/10], Validation Loss: 0.0025\nSaving Model! 0.0024928436832171736 < 0.0034352220000838426\nEpoch [7/10], Step [1/107], Loss: 0.0424\nEpoch [7/10], Step [11/107], Loss: 0.0891\nEpoch [7/10], Step [21/107], Loss: 0.0436\nEpoch [7/10], Step [31/107], Loss: 0.0503\nEpoch [7/10], Step [41/107], Loss: 0.0293\nEpoch [7/10], Step [51/107], Loss: 0.0427\nEpoch [7/10], Step [61/107], Loss: 0.0427\nEpoch [7/10], Step [71/107], Loss: 0.0437\nEpoch [7/10], Step [81/107], Loss: 0.0281\nEpoch [7/10], Step [91/107], Loss: 0.0415\nEpoch [7/10], Step [101/107], Loss: 0.0276\nEpoch [7/10], Loss: 0.0429\nEpoch [7/10], Validation Loss: 0.0010\nSaving Model! 0.0009627662883053107 < 0.0024928436832171736\nEpoch [8/10], Step [1/107], Loss: 0.0424\nEpoch [8/10], Step [11/107], Loss: 0.0522\nEpoch [8/10], Step [21/107], Loss: 0.0386\nEpoch [8/10], Step [31/107], Loss: 0.0172\nEpoch [8/10], Step [41/107], Loss: 0.0249\nEpoch [8/10], Step [51/107], Loss: 0.0363\nEpoch [8/10], Step [61/107], Loss: 0.0345\nEpoch [8/10], Step [71/107], Loss: 0.0240\nEpoch [8/10], Step [81/107], Loss: 0.0225\nEpoch [8/10], Step [91/107], Loss: 0.0294\nEpoch [8/10], Step [101/107], Loss: 0.0340\nEpoch [8/10], Loss: 0.0296\nEpoch [8/10], Validation Loss: 0.0009\nSaving Model! 0.0008753553695463202 < 0.0009627662883053107\nEpoch [9/10], Step [1/107], Loss: 0.0181\nEpoch [9/10], Step [11/107], Loss: 0.0226\nEpoch [9/10], Step [21/107], Loss: 0.0139\nEpoch [9/10], Step [31/107], Loss: 0.0163\nEpoch [9/10], Step [41/107], Loss: 0.0204\nEpoch [9/10], Step [51/107], Loss: 0.0172\nEpoch [9/10], Step [61/107], Loss: 0.0149\nEpoch [9/10], Step [71/107], Loss: 0.0258\nEpoch [9/10], Step [81/107], Loss: 0.0412\nEpoch [9/10], Step [91/107], Loss: 0.0282\nEpoch [9/10], Step [101/107], Loss: 0.0157\nEpoch [9/10], Loss: 0.0237\nEpoch [9/10], Validation Loss: 0.0006\nSaving Model! 0.0005691877860580266 < 0.0008753553695463202\nEpoch [10/10], Step [1/107], Loss: 0.0469\nEpoch [10/10], Step [11/107], Loss: 0.0239\nEpoch [10/10], Step [21/107], Loss: 0.0140\nEpoch [10/10], Step [31/107], Loss: 0.0095\nEpoch [10/10], Step [41/107], Loss: 0.0110\nEpoch [10/10], Step [51/107], Loss: 0.0302\nEpoch [10/10], Step [61/107], Loss: 0.0258\nEpoch [10/10], Step [71/107], Loss: 0.0386\nEpoch [10/10], Step [81/107], Loss: 0.0147\nEpoch [10/10], Step [91/107], Loss: 0.0185\nEpoch [10/10], Step [101/107], Loss: 0.0132\nEpoch [10/10], Loss: 0.0224\nEpoch [10/10], Validation Loss: 0.0009\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Testing on testing set","metadata":{"id":"Ytkn3WQWljqz"}},{"cell_type":"code","source":"transformer_model.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YE243onZM2Ff","outputId":"263e0dd8-0cbe-4255-b1ac-c5cd3b727a44","execution":{"iopub.status.busy":"2023-12-09T20:28:58.258612Z","iopub.execute_input":"2023-12-09T20:28:58.258923Z","iopub.status.idle":"2023-12-09T20:28:58.273051Z","shell.execute_reply.started":"2023-12-09T20:28:58.258895Z","shell.execute_reply":"2023-12-09T20:28:58.272108Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TransformerModel(\n  (mobilenet): Sequential(\n    (0): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n      (1): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (2): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (3): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (4): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (5): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (6): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (7): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (8): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (9): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (10): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (11): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n            (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (12): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (13): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (14): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (15): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (16): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (17): InvertedResidual(\n        (conv): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n            (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            (2): ReLU6(inplace=True)\n          )\n          (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (18): Conv2dNormActivation(\n        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU6(inplace=True)\n      )\n    )\n  )\n  (linear_cnn): Linear(in_features=81920, out_features=512, bias=True)\n  (pos_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.5, inplace=False)\n        (dropout2): Dropout(p=0.5, inplace=False)\n      )\n    )\n  )\n  (embed): Embedding(34, 512)\n  (transformer_decoder): TransformerDecoder(\n    (layers): ModuleList(\n      (0-2): 3 x TransformerDecoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (multihead_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n        )\n        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n        (dropout): Dropout(p=0.5, inplace=False)\n        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0.5, inplace=False)\n        (dropout2): Dropout(p=0.5, inplace=False)\n        (dropout3): Dropout(p=0.5, inplace=False)\n      )\n    )\n  )\n  (linear_vocab): Linear(in_features=512, out_features=34, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def generate_caption(transformer_model, vocab, frames, device, max_length=20):\n    start_token_idx = vocab['<sos>']\n    sequence = torch.tensor([[start_token_idx]], dtype=torch.long).to(device)\n\n    with torch.no_grad():\n        for _ in range(max_length):\n            output = transformer_model(frames, sequence)\n            next_word_idx = output.argmax(2)[:, -1].item()\n            sequence = torch.cat([sequence, torch.tensor([[next_word_idx]], dtype=torch.long).to(device)], dim=1)\n\n            if next_word_idx == vocab['<eos>']:\n                break\n\n    caption = indices_to_string(sequence[0], vocab)\n    return caption\n\n\ndef indices_to_string(indices, vocab):\n    if isinstance(indices, torch.Tensor):\n        indices = indices.tolist()\n\n    # Convert each index to its corresponding word\n    words = [vocab.get_itos()[idx] for idx in indices]\n\n    # Exclude special tokens like <start>, <end>, and <pad> from the final sentence\n    special_tokens = {'<sos>', '<eos>', '<pad>'}\n    filtered_words = [word for word in words if word not in special_tokens]\n\n    # Join the words to form a sentence\n    sentence = ' '.join(filtered_words)\n\n    return sentence\n\n","metadata":{"id":"qStAr0m1OUCx","execution":{"iopub.status.busy":"2023-12-09T20:29:03.647922Z","iopub.execute_input":"2023-12-09T20:29:03.648772Z","iopub.status.idle":"2023-12-09T20:29:03.657758Z","shell.execute_reply.started":"2023-12-09T20:29:03.648736Z","shell.execute_reply":"2023-12-09T20:29:03.656827Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"!pip install jiwer","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2AVVR8bqu9HJ","outputId":"fe203936-8960-4a41-b42f-22f72489f48b","execution":{"iopub.status.busy":"2023-12-09T20:29:06.087274Z","iopub.execute_input":"2023-12-09T20:29:06.087656Z","iopub.status.idle":"2023-12-09T20:29:19.328293Z","shell.execute_reply.started":"2023-12-09T20:29:06.087619Z","shell.execute_reply":"2023-12-09T20:29:19.327110Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Obtaining dependency information for jiwer from https://files.pythonhosted.org/packages/0d/4f/ee537ab20144811dd99321735ff92ef2b3a3230b77ed7454bed4c44d21fc/jiwer-3.0.3-py3-none-any.whl.metadata\n  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nRequirement already satisfied: rapidfuzz<4,>=3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (3.5.2)\nDownloading jiwer-3.0.3-py3-none-any.whl (21 kB)\nInstalling collected packages: jiwer\nSuccessfully installed jiwer-3.0.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import jiwer\n\ndef calculate_wer(reference, hypothesis):\n    return jiwer.wer(reference, hypothesis)\n\ntotal_wer = 0\nnum_samples = 0\n\nfor frames, captions in test_data_loader:\n    frames = frames.to(device)\n    captions = captions.to(device)\n\n    true_caption_indices = captions[0]\n    generated_caption = generate_caption(transformer_model, vocab, frames, device)\n\n    # Convert true caption indices to string\n    true_caption = indices_to_string(true_caption_indices, vocab)\n\n    # Calculate and accumulate WER\n    wer = calculate_wer(true_caption, generated_caption)\n    total_wer += wer\n    num_samples += 1\n\n    print(f\"True Caption: {true_caption}\")\n    print(f\"Generated Caption: {generated_caption}\")\n    print(f\"WER: {wer}\\n\")\n\n# Calculate average WER across the test set\naverage_wer = total_wer / num_samples\nprint(f\"Average WER on the Test Set: {average_wer}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"Q_Gs99WaM_Ng","outputId":"1312c0fe-67e9-4916-85b0-f97cd6ed0dfd","execution":{"iopub.status.busy":"2023-12-09T20:29:19.330765Z","iopub.execute_input":"2023-12-09T20:29:19.331175Z","iopub.status.idle":"2023-12-09T20:29:43.588060Z","shell.execute_reply.started":"2023-12-09T20:29:19.331135Z","shell.execute_reply":"2023-12-09T20:29:43.586913Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"True Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: الله اكبر\nGenerated Caption: لا شرك الله\nWER: 1.5\n\nTrue Caption: اسم الله\nGenerated Caption: الحمد الله\nWER: 0.5\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: السلام عليكم رحمة الله بركة\nGenerated Caption: الحمد الله\nWER: 0.8\n\nTrue Caption: موضوع دراسة لغة الاشارة العربية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: لا شرك الله\nGenerated Caption: لا شرك الله\nWER: 0.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: السلام عليكم رحمة الله بركة\nGenerated Caption: الحمد الله\nWER: 0.8\n\nTrue Caption: كلمات اليوم متفرقة في الدين\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: ايضا كلمات عادية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: الله اكبر\nGenerated Caption: لا شرك الله\nWER: 1.5\n\nTrue Caption: جميع الصم العرب السامع\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: جميع الصم العرب السامع\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: الحمد الله\nGenerated Caption: الحمد الله\nWER: 0.0\n\nTrue Caption: كلمات اليوم متفرقة في الدين\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: السلام عليكم رحمة الله بركة\nGenerated Caption: لا شرك الله\nWER: 0.8\n\nTrue Caption: كلمات اليوم متفرقة في الدين\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: كلمات اليوم متفرقة في الدين\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: موضوع دراسة لغة الاشارة العربية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: لا شرك الله\nGenerated Caption: لا شرك الله\nWER: 0.0\n\nTrue Caption: الله اكبر\nGenerated Caption: لا شرك الله\nWER: 1.5\n\nTrue Caption: موضوع دراسة لغة الاشارة العربية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: ايضا كلمات عادية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: جميع الصم العرب السامع\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: موضوع دراسة لغة الاشارة العربية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: السلام عليكم رحمة الله بركة\nGenerated Caption: الحمد الله\nWER: 0.8\n\nTrue Caption: ايضا كلمات عادية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: كلمات اليوم متفرقة في الدين\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: كلمات اليوم متفرقة في الدين\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: لا شرك الله\nGenerated Caption: لا شرك الله\nWER: 0.0\n\nTrue Caption: اسم الله\nGenerated Caption: الحمد الله\nWER: 0.5\n\nTrue Caption: جميع الصم العرب السامع\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: اسم الله\nGenerated Caption: الحمد الله\nWER: 0.5\n\nTrue Caption: الله اكبر\nGenerated Caption: لا شرك الله\nWER: 1.5\n\nTrue Caption: الحمد الله\nGenerated Caption: الحمد الله\nWER: 0.0\n\nTrue Caption: ايضا كلمات عادية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: اسم الله\nGenerated Caption: الحمد الله\nWER: 0.5\n\nTrue Caption: لا شرك الله\nGenerated Caption: لا شرك الله\nWER: 0.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: الحمد الله\nGenerated Caption: الحمد الله\nWER: 0.0\n\nTrue Caption: جميع الصم العرب السامع\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: جميع الصم العرب السامع\nGenerated Caption: الحمد الله\nWER: 1.0\n\nTrue Caption: اسم الله\nGenerated Caption: الحمد الله\nWER: 0.5\n\nTrue Caption: الحمد الله\nGenerated Caption: الحمد الله\nWER: 0.0\n\nTrue Caption: الحمد الله\nGenerated Caption: الحمد الله\nWER: 0.0\n\nTrue Caption: لا شرك الله\nGenerated Caption: لا شرك الله\nWER: 0.0\n\nTrue Caption: السلام عليكم رحمة الله بركة\nGenerated Caption: لا شرك الله\nWER: 0.8\n\nTrue Caption: موضوع دراسة لغة الاشارة العربية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: موضوع دراسة لغة الاشارة العربية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: لا شرك الله\nGenerated Caption: لا شرك الله\nWER: 0.0\n\nTrue Caption: السلام عليكم رحمة الله بركة\nGenerated Caption: الحمد الله\nWER: 0.8\n\nTrue Caption: ايضا كلمات عادية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: ايضا كلمات عادية\nGenerated Caption: لا شرك الله\nWER: 1.0\n\nTrue Caption: الله اكبر\nGenerated Caption: لا شرك الله\nWER: 1.5\n\nTrue Caption: الله اكبر\nGenerated Caption: لا شرك الله\nWER: 1.5\n\nTrue Caption: اسم الله\nGenerated Caption: الحمد الله\nWER: 0.5\n\nTrue Caption: الحمد الله\nGenerated Caption: الحمد الله\nWER: 0.0\n\nTrue Caption: اليوم اقدم انتم برنامج اخر\nGenerated Caption: الحمد الله\nWER: 1.0\n\nAverage WER on the Test Set: 0.7999999999999999\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Average WER on the Test Set: 0.79999999999\n\nThis appears to be very bad results. Although we are performing well on both the training and validation set, it seems the testing set is giving bad results. this could be due to the vast difference in distribution between train and test sets.","metadata":{}}]}